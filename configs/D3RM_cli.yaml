seed_everything: 23
# ckpt_path : ./checkpoints/2024-10-30T10-02-10/last.ckpt
debug: False
wandb: True

trainer:
  benchmark: True
  gradient_clip_algorithm: norm
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  devices: 2
  accelerator: gpu
  strategy: ddp
  log_every_n_steps: 10
  check_val_every_n_epoch: null
  val_check_interval: 5000
  # enable_checkpointing: True
  max_steps: 8000000

  # callbacks:
  #   - class_path: pytorch_lightning.callbacks.ModelCheckpoint
  #     init_args:
  #       monitor: metric/note-with-offsets/f1
  #       mode: max
  #       save_top_k: 5
  #       save_last: True
  #       save_weights_only: False
  #       filename: '{step}-{metric/note-with-offsets/f1:.4f}'
  #       dirpath: checkpoints

model:
  encoder_parameters: pretrained
  pretrained_encoder_path: model_170k_0.9063_nonar.pt
  freeze_encoder: False
  test_save_path: './results/'
  use_ema: True
  ema_decay: 0.99
  ema_update_interval: 25

  label_seq_len: 27544
  diffusion_step: 100
  gamma_bar_T: 0.9
  auxiliary_loss_weight: 5.0e-4
  adaptive_auxiliary_loss: true
  mask_weight: [1, 1]
  classifier_free_guidance: False
  onset_suppress_sample: False
  onset_weight_kl: False
  no_mask: False
  sample_from_fully_masked: True
  reverse_sampling: True

  encoder:
    class_path: transcription.encoder.ARModel
    init_args:
      use_vel: False

  decoder:  
    class_path: transcription.decoder.TransModel
    init_args:
      label_embed_dim: 4
      lstm_dim: 48
      n_layers: 8
      window: [3,3,3,3,3,3,3,3]
      dilation: [1,2,4,8,1,2,4,8]
      condition_method: self
      diffusion_step: 100
      timestep_type: adalayernorm
      natten_direction: 2d
      spatial_size: [313,88]
      num_state: 5
      classifier_free_guidance: False

  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1.0e-3
      weight_decay: 4.5e-2
      betas: [0.9, 0.96]

  scheduler:
    monitor: train/diffusion_loss
    factor: 0.8
    patience: 25000
    min_lr: 1.0e-5
    threshold: 1.0e-1
    threshold_mode: rel
    # warmup_lr: 4.5e-4
  # warmup: 1000


data:
  data_dir: './data/maestro-v3.0.0'
  batch_size: 4
  num_workers: 8
  # train: True
  # validation: True
  # test: True
  # predict: False
  train_seq_len: 160256
  valid_seq_len: 160256
  # sampling_rate: 16000
  hop_size: 512